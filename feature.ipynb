{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5ac5f7a853dab503f6e622d281e57459330024aad0ea00c1ac36dd2cd29fe2ec"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "1. 特征工程的目的是从原始数据中提取特征以供算法和模型使用。sklearn库包含众多函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#load data from sklearn \n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "#iris数据有特征矩阵iris.data，尺寸为【150，4】，标签iris.target, iris.feature_names和iris.target_names"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "2. 数据预处理\n",
    " 2.1 无量纲化，包括标准化（前提为正太分布，标准化后为标准正态分布）；区间缩放，比如除以最值，区间变为【0,1】或【-1，1】\n",
    " 注：如果对输出结果范围有要求，用区间缩放；如果数据较为稳定，不存在极端的最大最小值，用区间缩放；如果数据存在异常值和较多噪音，用标 准化，可以间接通过中心化避免异常值和极端值的影响"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original iris No.1-3: \n [[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]]\nafter standardscaler iris No.1-3: \n [[ 1.22474487e+00  1.29777137e+00  7.07106781e-01 -1.00000000e+00]\n [ 5.43895982e-15 -1.13554995e+00  7.07106781e-01 -1.00000000e+00]\n [-1.22474487e+00 -1.62221421e-01 -1.41421356e+00 -1.00000000e+00]]\nafter maxscaler iris No.1-3: \n [[1.  1.  1.  0. ]\n [0.5 0.  1.  0. ]\n [0.  0.4 0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "print('original iris No.1-3: \\n', iris.data[0:3])\n",
    "#标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris_standard=StandardScaler().fit_transform(iris.data[0:3])\n",
    "print('after standardscaler iris No.1-3: \\n', iris_standard)\n",
    "#区间缩放\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "iris_MinMax=MinMaxScaler().fit_transform(iris.data[0:3])\n",
    "print('after maxscaler iris No.1-3: \\n', iris_MinMax)\n"
   ]
  },
  {
   "source": [
    "此外，还有归一化，他是按照特征矩阵的行处理数据（注意标准化和区间缩放都是根据列向量，即不同样本的同一个特征变量）.归一化的目的是在点乘运算或是其他和函数计算相似性时，拥有统一的标准，也就是转换为“单位向量”。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after normalizer iris No.1-3: \n [[0.80377277 0.55160877 0.22064351 0.0315205 ]\n [0.82813287 0.50702013 0.23660939 0.03380134]\n [0.80533308 0.54831188 0.2227517  0.03426949]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer \n",
    "iris_norm = Normalizer().fit_transform(iris.data[0:3])\n",
    "print('after normalizer iris No.1-3: \\n', iris_norm)"
   ]
  },
  {
   "source": [
    "2.2 对定量特征二值化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after binarizer iris No.1-3: \n [[1. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "iris_Binary=Binarizer(threshold=3).fit_transform(iris.data[:3])\n",
    "print('after binarizer iris No.1-3: \\n', iris_Binary)"
   ]
  },
  {
   "source": [
    "2.3 One-hot编码，把定性变量转换为one-hot的哑编码 eg. (1,2,3)->[【1,0,0】【0,1,0】【0,0,1】]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "iris_target_OneHot= OneHotEncoder().fit_transform(iris.target.reshape(-1,1))\n",
    "print(iris_target_OneHot.shape)"
   ]
  },
  {
   "source": [
    "2.4 缺失值计算。 使用Imputer计算缺失值，参数missing_value为缺失值的表示形式（默认为NaN),strategy为填充方式，默认为均值mean\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nan nan nan nan] (151, 4) (150, 4)\n",
      "[5.84333333 3.05733333 3.758      1.19933333]\n",
      "/home/chumiao/.conda/envs/pytorch/lib/python3.6/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "iris151 = np.vstack((np.array([np.nan,np.nan,np.nan,np.nan]),iris.data))\n",
    "print(iris151[0],iris151.shape,iris.data.shape)\n",
    "iris151new = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0, verbose=0, copy=True).fit_transform(iris151)\n",
    "print(iris151new[0])"
   ]
  },
  {
   "source": [
    "2.5 数据转换，包括多项式转换，任意自定义函数log,exp转换\n "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 4)\n(3, 15)\n(3, 35)\n(3, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures,FunctionTransformer\n",
    "#多项式转换，利用现有的变量相乘增加为指定阶数的新变量\n",
    "iris_Poly2=PolynomialFeatures(degree=2).fit_transform(iris.data[:3])\n",
    "iris_Poly3=PolynomialFeatures(degree=3).fit_transform(iris.data[:3])\n",
    "print(iris.data[:3].shape)\n",
    "print(iris_Poly2.shape)\n",
    "print(iris_Poly3.shape)\n",
    "#自定义转换函数\n",
    "iris_log = FunctionTransformer(func=np.log1p).fit_transform(iris.data[:3])\n",
    "print(iris_log.shape)"
   ]
  },
  {
   "source": [
    "3. 特征选择。\n",
    "在预处理之后，对有意义的特征进行筛选，主要考察特征的①发散性（如果不发散var=0,该特征无意义）；②与目标的相关性\n",
    "方法为3中：① Filter (根据相关性和发散性按照阈值来选择)；② Wrapper (递归特征消除法) ③ Embedded (使用机器学习算法和模型，按照特征的权值系数选择特征，高级的Filter)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.1 Filter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#方差选择法\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "iris_Var_Select = VarianceThreshold(threshold=3).fit_transform(iris.data)\n",
    "print(iris_Var_Select.shape)\n",
    "iris_Var_Select = VarianceThreshold(threshold=0.5).fit_transform(iris.data)\n",
    "print(iris_Var_Select.shape)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(150, 1)\n(150, 3)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单变量分析法 univariate feature selection,基于在统计中特征与输出的相关系数、回归系数、互信息等进行筛选\n",
    "# - 其中筛选函数有按保留个数筛选SelectKBest,按百分比筛选SelectPercentile，按关注的统计特性(false positive rate, false discovery rate )筛选SelectFpr,SelctFdr\n",
    "# - 统计特征：For regression: f_regression, mutual_info_regression; For classification: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from scipy.stats import pearsonr\n",
    "a = lambda X, Y: np.abs((np.array(list(map(lambda x:pearsonr(x, Y), X.T))).T)[0]) \n",
    "#a为lambda函数，计算特征矩阵m个参数X.T与Y的相关性，得到二维矩阵(m,2),其中第一维代表相关系数，第二维代表显著性p；SelectKBest只能是一维度的输出结果，所有取了[0]\n",
    "iris_Person = SelectKBest(a, k=2).fit(iris.data, iris.target) #这里先使用了fit再调用get_support,从而得到保留下来的feature索引\n",
    "print(iris_Person.get_support(indices=True))\n",
    "iris_Person_new = SelectKBest(a, k=2).fit_transform(iris.data, iris.target) #fit_transform返回特征过滤后保留下的特征数据集。\n",
    "print(iris_Person_new.shape)\n",
    "iris_chi2 = SelectKBest(chi2, k=2).fit(iris.data, iris.target) #经典的卡方检验是检验定性自变量对定性因变量的相关性\n",
    "print(iris_chi2.get_support(indices=True))\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 74,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2 3]\n(150, 2)\n[2 3]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.4 0.2]\n [1.4 0.2]\n [1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "#互信息法\n",
    "from minepy import MINE\n",
    "def mic(x,y):\n",
    "    m = MINE()\n",
    "    m.compute_score(x,y)\n",
    "    return m.mic()\n",
    "a = lambda X,Y: np.array(list(map(lambda x: mic(x,Y), X.T))).T\n",
    "iris_MultInfo = SelectKBest(a,k=2).fit_transform(iris.data,iris.target)\n",
    "print(iris_MultInfo[:3])\n"
   ]
  },
  {
   "source": [
    "3.2 Wrapper\n",
    "递归消除特征法使用一个基模型(自己可以选择）来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练，直到满足剩余的特征个数。使用feature_selection库的RFE类/RFECV类（带cross-validation)来选择特征的代码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[False False  True  True]\n[2 3 1 1]\n(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "estimator = SVR(kernel='linear')\n",
    "selector = RFE(estimator,n_features_to_select=2,step=1)\n",
    "selector = selector.fit(iris.data,iris.target)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "iris_selector_data = selector.transform(iris.data)\n",
    "print(iris_selector_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(150, 4) (150,)\noptimal num of features 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC #svc classification ,svr regression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "estimator = SVC(kernel='linear')\n",
    "selector = RFECV(estimator,step=1,cv=StratifiedKFold(2),scoring='accuracy',min_features_to_select=1)\n",
    "selector = selector.fit(iris.data,iris.target)\n",
    "print(\"optimal num of features\",selector.n_features_) #自动计算optimal的feature num，不需要像REF一样指定"
   ]
  },
  {
   "source": [
    "3.3 SelectFromModel 像是Variance_Threshold 和 RFE的一个综合。利用任何estimator来对每个特征进行评分，然后可以根据threshold来选择保留的特征；除了设置阈值外，还可以设定max_features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.          0.         -0.18016819  0.        ]\n [-0.03183986  0.          0.          0.        ]\n [-0.00677759  0.          0.          0.        ]]\n1e-05 [ True False  True False]\n[[ 0.03366372  0.24569333 -0.46437192 -0.20710127]\n [-0.08039609 -0.17543666  0.09803087  0.00640839]\n [-0.18404126 -0.22424632  0.29110621  0.18614789]]\n0.5491609826547749 [False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "# L1和L2惩罚的estimator:In particular, sparse estimators useful for this purpose are the Lasso for regression, and of LogisticRegression and LinearSVC for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "estimator_l1 = LogisticRegression(penalty='l1',C=0.01)\n",
    "estimator_l2 = LogisticRegression(penalty='l2',C=0.01)\n",
    "Selector_l1 = SelectFromModel(estimator_l1).fit(iris.data,iris.target)\n",
    "print(Selector_l1.estimator_.coef_)\n",
    "print(Selector_l1.threshold_, Selector_l1.get_support())\n",
    "\n",
    "Selector_l2 = SelectFromModel(estimator_l2).fit(iris.data,iris.target)\n",
    "print(Selector_l2.estimator_.coef_)\n",
    "print(Selector_l2.threshold_, Selector_l2.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.25 [False False  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Tree-based estimators: sklearn.tree or sklearn.ensemble \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "# print(clf.feature_importances_)\n",
    "selector = SelectFromModel(clf).fit(iris.data,iris.target)\n",
    "print(selector.threshold_,selector.get_support())"
   ]
  },
  {
   "source": [
    "4. 降维。 在特征选择过后，保留下来的特征仍然很多，矩阵运算过大训练时间长。常见的降维包括主成分分析法（PCA）和线性判别分析（LDA）：线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]]\n[[-2.68412563  0.31939725]\n [-2.71414169 -0.17700123]\n [-2.88899057 -0.14494943]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "data_PCA = PCA(n_components=2).fit_transform(iris.data)\n",
    "print(iris.data[:3])\n",
    "print(data_PCA[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 8.06179978  0.30042062]\n [ 7.12868772 -0.78666043]\n [ 7.48982797 -0.26538449]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "data_LDA = LDA(n_components=2).fit_transform(iris.data,iris.target)\n",
    "print(data_LDA[:3])"
   ]
  }
 ]
}